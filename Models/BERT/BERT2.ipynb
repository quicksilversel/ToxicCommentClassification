{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4LdFAQdLniZ",
        "outputId": "772b4900-882d-4fc4-b3de-64c282a96925"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\programmering\\KAIST\\CS470\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\jonas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import transformers\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "SEED = 1\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WafDC1NRLt_L"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../../Data/train.csv')\n",
        "test = pd.read_csv('../../Data/test.csv')\n",
        "test_labels = pd.read_csv('../../Data/test_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "At0u7pwFqlrG"
      },
      "outputs": [],
      "source": [
        "LABELS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "N_LABELS = len(LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "upgcowLEMOjN"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "def preprocessComments(comment):\n",
        "    # Convert to lowercase, important for this bertmodel\n",
        "    comment = comment.lower()\n",
        "\n",
        "    # Remove leading and trailing spaces\n",
        "    comment = comment.strip()\n",
        "\n",
        "    # Remove consecutive spaces      \n",
        "    comment = re.sub(r' +', ' ', comment)\n",
        "\n",
        "    # Remove Newlines\n",
        "    comment = re.sub(r'\\n', ' ', comment)\n",
        "\n",
        "    return comment\n",
        "\n",
        "train.comment_text = train.comment_text.map(preprocessComments)\n",
        "train_y = train[LABELS].values\n",
        "test.comment_text = test.comment_text.map(preprocessComments)\n",
        "\n",
        "if train['comment_text'].isnull().values.any():\n",
        "  raise Exception(\"Missing data\")\n",
        "if test['comment_text'].isnull().values.any():\n",
        "  raise Exception(\"Missing data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpgKN3wa0ohr",
        "outputId": "4d4ac090-3c7d-4971-d8d5-0c419d7d93b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(63978, 8)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "63978"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_filtered = pd.merge(test, test_labels)\n",
        "test_filtered = test_filtered.drop(test_filtered.index[test_filtered['toxic'] == -1])\n",
        "comments_list = train['comment_text'].tolist()\n",
        "test_comments_list = test_filtered['comment_text'].tolist()\n",
        "print(test_filtered.shape)\n",
        "len(test_comments_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhqII4yQqcyW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMsuJrSRMTtd",
        "outputId": "f2c9f66a-c125-4de5-821b-bc313fe2264d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 570/570 [00:00<00:00, 71.0kB/s]\n",
            "Downloading: 100%|██████████| 511M/511M [24:53<00:00, 359kB/s]   \n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Downloading: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
            "Downloading: 100%|██████████| 226k/226k [00:02<00:00, 98.4kB/s] \n",
            "Downloading: 100%|██████████| 455k/455k [00:05<00:00, 87.5kB/s] \n"
          ]
        }
      ],
      "source": [
        "bert_name = \"bert-base-uncased\"\n",
        "\n",
        "bert_model = transformers.TFAutoModel.from_pretrained(bert_name)\n",
        "\n",
        "tokenizer = transformers.BertTokenizerFast.from_pretrained(\n",
        "    pretrained_model_name_or_path=bert_name, \n",
        "    config=transformers.BertConfig.from_pretrained(bert_name))\n",
        "\n",
        "TOKENS_MAX_LENGTH = 120\n",
        "\n",
        "def create_tokenizer(comments):\n",
        "  return tokenizer(\n",
        "    text=comments,\n",
        "    padding='longest', \n",
        "    truncation='longest_first',\n",
        "    max_length=TOKENS_MAX_LENGTH,\n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True)\n",
        "  \n",
        "train_tokenizer = create_tokenizer(comments_list)\n",
        "test_tokenizer = create_tokenizer(test_comments_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt9JANJcMVtS"
      },
      "outputs": [],
      "source": [
        "att_mask = tf.keras.layers.Input(shape=(TOKENS_MAX_LENGTH,), name='attention_mask', dtype='int32') \n",
        "input_ids = tf.keras.layers.Input(shape=(TOKENS_MAX_LENGTH,), name='input_ids', dtype='int32')\n",
        "input = {'attention_mask': att_mask, 'input_ids': input_ids}\n",
        "x = bert_model.bert(input)\n",
        "\n",
        "x = tf.keras.layers.GlobalAveragePooling1D()(x[0])\n",
        "x = tf.keras.layers.Dense(N_LABELS, activation='sigmoid')(x)\n",
        "model = tf.keras.models.Model(input, x)\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, decay=1e-5),\n",
        "    metrics=['acc']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puxYIaTdMg_Y",
        "outputId": "957ac809-0427-4c00-b302-00f458b38af7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "1995/1995 [==============================] - 3353s 2s/step - loss: 0.0499 - acc: 0.9146 - val_loss: 0.0457 - val_acc: 0.9756\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f89d041eb50>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(\n",
        "    {'attention_mask': train_tokenizer['attention_mask'], 'input_ids': train_tokenizer['input_ids']},\n",
        "    train_y,\n",
        "    validation_split=0.2,\n",
        "    epochs=1,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "M2-gSkTuEA68"
      },
      "outputs": [],
      "source": [
        "# Save model weights\n",
        "\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/bert.h5') \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAEEx7NcGZiI",
        "outputId": "9bf81dc2-ac28-43b2-d9f8-47dd71f7d296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 516s 513ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(\n",
        "    {'attention_mask': test_tokenizer['attention_mask'], 'input_ids': test_tokenizer['input_ids']},\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d3boU3018-Y",
        "outputId": "a5fc9037-b3a6-4210-c0fd-714d3dd11674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toxic :\n",
            "0.6588192017656797\n",
            "severe_toxic :\n",
            "0.36594202898550726\n",
            "obscene :\n",
            "0.6325704059527174\n",
            "threat :\n",
            "0.26732673267326734\n",
            "insult :\n",
            "0.6399262332872291\n",
            "identity_hate :\n",
            "0.4471243042671614\n",
            "Average f1-score: 0.5019514844885937\n"
          ]
        }
      ],
      "source": [
        "avg = 0\n",
        "for i, label in enumerate(LABELS):\n",
        "    print(label, \":\")\n",
        "    pb = predictions[:, i] >= 0.5\n",
        "    score = f1_score(test_filtered[label], pb)\n",
        "    print(score)\n",
        "    avg += score\n",
        "\n",
        "avg /= N_LABELS\n",
        "print(\"Average f1-score:\", avg)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 ('env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "f0756d1539b148037f3e5ac67af8f49cb7b47ce7ee88e1da2562ab72b6a7a25e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
