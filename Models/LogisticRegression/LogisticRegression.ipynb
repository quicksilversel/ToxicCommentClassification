{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470_LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Word Embedding Method : TF-IDF"
      ],
      "metadata": {
        "id": "s1FWZUGcG9AW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DeX6e8Mn4K5"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords                  # module for stop words that come with NLTK\n",
        "from nltk.stem.wordnet import WordNetLemmatizer    # module for lemmatization\n",
        "from nltk import word_tokenize, pos_tag            # tokenization and Part of Speech tagging\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english') # English stopwords\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulation"
      ],
      "metadata": {
        "id": "XNxrLWW9HGTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/train.csv')\n",
        "test_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test.csv')\n",
        "test_label_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test_labels.csv')\n",
        "\n",
        "# use only rows that were used for scoring\n",
        "test_label_data = test_label_data.loc[test_label_data['toxic']!=-1]\n",
        "test = test_label_data.merge(test_data, on='id', how=\"inner\")"
      ],
      "metadata": {
        "id": "NSeqkgSTTXDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the comments\n",
        "\n",
        "# From a string, make text lowercase, remove hyperlinks, punctuation, word containing numbers, stopwords.\n",
        "# Input : a list of string\n",
        "# Output : a list of tokens stored in a generator (yield)\n",
        "\n",
        "def preprocess(corpus):\n",
        "\n",
        "    for text in corpus:\n",
        "\n",
        "        text = text.lower()                                               # Lowercase\n",
        "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)                   # Remove links\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)   # Remove punctuation\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)                               # Remove words containing numbers\n",
        "    \n",
        "        yield ' '.join([word for word in text.split(' ') if word not in stopwords_english]) # Return a generator \n",
        "\n",
        "# proprocessed train dataset\n",
        "clean_comments = list(preprocess(train_data['comment_text']))\n",
        "# preprocessed test dataset\n",
        "test_clean_comments = list(preprocess(test['comment_text']))\n",
        "\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']"
      ],
      "metadata": {
        "id": "X54vZWXoTRZo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding : TF-IDF\n",
        "tfidf_vec = TfidfVectorizer(min_df=1,max_df=0.9)\n",
        "\n",
        "X_train = tfidf_vec.fit_transform(clean_comments)\n",
        "X_test = tfidf_vec.transform(test_clean_comments)\n",
        "Y_train = train_data[labels]\n",
        "\n",
        "submission = pd.DataFrame.from_dict({'id': test['id']})"
      ],
      "metadata": {
        "id": "BEPtw8GRRdHh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "oa7xg5UFK7oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model and calculate f1 scores\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "for label in labels:\n",
        "\n",
        "    # Model\n",
        "    LR = LogisticRegression(solver='saga', n_jobs=-1, C=0.5)\n",
        "    \n",
        "    # Calculate F1-score\n",
        "    f1_score = np.mean(cross_val_score(LR, X_train, Y_train[label], cv=3, n_jobs=-1, scoring='f1'))\n",
        "    f1_scores.append(f1_score)\n",
        "    print(\"F1 score for class {} is {}\".format(label, f1_score))\n",
        "    \n",
        "    # re-learn & predict\n",
        "    LR.fit(X_train, Y_train[label])  \n",
        "    submission[label] = LR.predict_proba(X_test)[:, 1] # predict\n",
        "    \n",
        "print(\"Average F1 score: {}\".format(np.mean(f1_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oSx3UgJHQmZ",
        "outputId": "f6f6c38d-6362-43c3-9a2b-eb19309e9387"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for class toxic is 0.6500233714915494\n",
            "F1 score for class severe_toxic is 0.2670401910132863\n",
            "F1 score for class obscene is 0.6779626665264592\n",
            "F1 score for class threat is 0.07482626248557024\n",
            "F1 score for class insult is 0.5520726719300045\n",
            "F1 score for class identity_hate is 0.15913335650561927\n",
            "Average F1 score: 0.39684308665874823\n"
          ]
        }
      ]
    }
  ]
}