{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "Word Embedding Method : TF-IDF"
      ],
      "metadata": {
        "id": "s1FWZUGcG9AW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-DeX6e8Mn4K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff03dc8-55b9-4577-b37b-1146d1b18b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords                  # module for stop words that come with NLTK\n",
        "from nltk.stem.wordnet import WordNetLemmatizer    # module for lemmatization\n",
        "from nltk import word_tokenize, pos_tag            # tokenization and Part of Speech tagging\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english') # English stopwords\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulation"
      ],
      "metadata": {
        "id": "XNxrLWW9HGTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/train.csv')\n",
        "test_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test.csv')\n",
        "test_label_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test_labels.csv')\n",
        "\n",
        "# use only rows that were used for scoring\n",
        "test_label_data = test_label_data.loc[test_label_data['toxic']!=-1]\n",
        "test = test_label_data.merge(test_data, on='id', how=\"inner\")"
      ],
      "metadata": {
        "id": "NSeqkgSTTXDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf72fa8-c307-431e-8fa4-e1d94f77cf99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the comments\n",
        "\n",
        "# From a string, make text lowercase, remove hyperlinks, punctuation, word containing numbers, stopwords.\n",
        "# Input : a list of string\n",
        "# Output : a list of tokens stored in a generator (yield)\n",
        "\n",
        "def preprocess(corpus):\n",
        "\n",
        "    for text in corpus:\n",
        "\n",
        "        text = text.lower()                                               # Lowercase\n",
        "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)                   # Remove links\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)   # Remove punctuation\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)                               # Remove words containing numbers\n",
        "    \n",
        "        yield ' '.join([word for word in text.split(' ') if word not in stopwords_english]) # Return a generator \n",
        "\n",
        "# proprocessed train dataset\n",
        "clean_comments = list(preprocess(train_data['comment_text']))\n",
        "# preprocessed test dataset\n",
        "test_clean_comments = list(preprocess(test['comment_text']))\n",
        "\n",
        "labels = ['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']"
      ],
      "metadata": {
        "id": "X54vZWXoTRZo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding : TF-IDF\n",
        "tfidf_vec = TfidfVectorizer(min_df=1,max_df=0.9)\n",
        "\n",
        "X_train = tfidf_vec.fit_transform(clean_comments)\n",
        "X_test = tfidf_vec.transform(test_clean_comments)\n",
        "Y_train = train_data[labels]\n",
        "\n",
        "df_classification = pd.DataFrame() \n",
        "df_classification['id'] = test['id']\n",
        "df_classification['comment_text'] = test['comment_text']"
      ],
      "metadata": {
        "id": "BEPtw8GRRdHh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "oa7xg5UFK7oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model and calculate accuracy\n",
        "scores = []\n",
        "\n",
        "for label in labels:\n",
        "\n",
        "    # Model\n",
        "    LR = LogisticRegression(solver='saga', n_jobs=-1, C=0.5)\n",
        "    \n",
        "    # Calculate F1-score\n",
        "    score = np.mean(cross_val_score(LR, X_train, Y_train[label], cv=3, n_jobs=-1, scoring='accuracy'))\n",
        "    scores.append(score)\n",
        "    print(\"Accuracy for class {} is {}\".format(label, score))\n",
        "    \n",
        "    LR.fit(X_train, Y_train[label])  \n",
        "    df_classification[label] = LR.predict_proba(X_test)\n",
        "    \n",
        "print(\"Average accuracy: {}\".format(np.mean(scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oSx3UgJHQmZ",
        "outputId": "5bfc3e64-18d2-4e1b-b307-894901a82fc0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class toxic is 0.9488628905011796\n",
            "Accuracy for class severe_toxic is 0.9904368592197064\n",
            "Accuracy for class obscene is 0.9733096874590889\n",
            "Accuracy for class threat is 0.9970796704144117\n",
            "Accuracy for class insult is 0.9670679556928617\n",
            "Accuracy for class identity_hate is 0.9916212849613188\n",
            "Average accuracy: 0.9780630580414278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate f1 scores\n",
        "\n",
        "f1_scores = []\n",
        "\n",
        "for label in labels:\n",
        "\n",
        "    # Model\n",
        "    LR = LogisticRegression(solver='saga', n_jobs=-1, C=0.5)\n",
        "    \n",
        "    # Calculate F1-score\n",
        "    f1_score = np.mean(cross_val_score(LR, X_train, Y_train[label], cv=3, n_jobs=-1, scoring='f1'))\n",
        "    f1_scores.append(f1_score)\n",
        "    print(\"F1 score for class {} is {}\".format(label, f1_score))\n",
        "    \n",
        "print(\"Average F1 score: {}\".format(np.mean(f1_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HugzduIxoCTh",
        "outputId": "19c8b7a2-bca0-4b5e-ad2d-724e9cfb88cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for class toxic is 0.6499655983851421\n",
            "F1 score for class severe_toxic is 0.2670401910132863\n",
            "F1 score for class obscene is 0.6779626665264592\n",
            "F1 score for class threat is 0.07482626248557024\n",
            "F1 score for class insult is 0.5520726719300045\n",
            "F1 score for class identity_hate is 0.15913335650561927\n",
            "Average F1 score: 0.39683345780768037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create CSV file\n",
        "\n",
        "df_classification.to_csv(\"/content/gdrive/My Drive/Colab Notebooks/LR_precisions.csv\", index=False)"
      ],
      "metadata": {
        "id": "l0C1y-_XCynB"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}