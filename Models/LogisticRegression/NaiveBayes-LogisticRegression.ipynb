{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NB-LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression \n",
        "\n",
        "Word Embedding Method 1 : BOW\n",
        "\n",
        "Word Embedding **Method** 2 : TF-IDF"
      ],
      "metadata": {
        "id": "ts9OrTpv5NjR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DeX6e8Mn4K5"
      },
      "outputs": [],
      "source": [
        "# libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import itertools as it\n",
        "import pickle\n",
        "import os\n",
        "from  pathlib import Path\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords                  \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# English stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords_english = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Manipulation"
      ],
      "metadata": {
        "id": "0KMOkOoB8n1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "train_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/train.csv')\n",
        "test_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test.csv')\n",
        "test_label_data = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/Data/test_labels.csv')\n",
        "\n",
        "# use only rows that were used for scoring\n",
        "test_label_data = test_label_data.loc[test_label_data['toxic']!=-1]\n",
        "test = test_label_data.merge(test_data, on='id', how=\"inner\")"
      ],
      "metadata": {
        "id": "NSeqkgSTTXDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the comments\n",
        "\n",
        "# From a string, make text lowercase, remove hyperlinks, punctuation, word containing numbers, stopwords.\n",
        "# Input : a list of string\n",
        "# Output : a list of tokens stored in a generator (yield)\n",
        "\n",
        "def preprocess(corpus):\n",
        "\n",
        "    for text in corpus:\n",
        "\n",
        "        text = text.lower()                                               # Lowercase\n",
        "        text = re.sub(r'https?://[^\\s\\n\\r]+', '', text)                   # Remove links\n",
        "        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)   # Remove punctuation\n",
        "        text = re.sub('\\w*\\d\\w*', '', text)                               # Remove words containing numbers\n",
        "    \n",
        "        yield ' '.join([word for word in text.split(' ') if word not in stopwords_english]) # Return a generator \n",
        "\n",
        "# proprocessed train dataset\n",
        "clean_comments = list(preprocess(train_data['comment_text']))\n",
        "# preprocess test dataset\n",
        "test_clean_comments = list(preprocess(test['comment_text']))\n",
        "\n",
        "# classification labels\n",
        "label = train_data[['toxic', 'severe_toxic', 'obscene', 'threat','insult', 'identity_hate']]"
      ],
      "metadata": {
        "id": "X54vZWXoTRZo"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "RBghHi4v7Cld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that returns vector of Naive Bayes probabilities with smoothing (n_words,1)\n",
        "\n",
        "def probNB(bow,label,cat):\n",
        "\n",
        "    p = np.array(bow[label==cat].sum(axis=0))\n",
        "\n",
        "    return np.transpose((p+1) / (p.sum() + bow.shape[1]))"
      ],
      "metadata": {
        "id": "BEPtw8GRRdHh"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that returns the log likelihood of a document (n_doc,1)\n",
        "\n",
        "def get_model(bow,label):\n",
        "\n",
        "    log = np.log(probNB(bow,label,1)/probNB(bow,label,0))\n",
        "    m = bow.dot(log)\n",
        "    model = LogisticRegression().fit(m,label)\n",
        "    return model, log"
      ],
      "metadata": {
        "id": "qp1RoapnrZ-n"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. BOW + NB + Logistic Regression"
      ],
      "metadata": {
        "id": "WLPoi3zo86QK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding method 1 : Bag of Words\n",
        "\n",
        "# Filter words that appear more than 30% but less than 90% of the document\n",
        "vectorizer = CountVectorizer(min_df=3,max_df=0.9) \n",
        "\n",
        "# BOW for train dataset\n",
        "bow = vectorizer.fit_transform(clean_comments)\n",
        "# BOW for test dataset\n",
        "bow_test = vectorizer.transform(test_clean_comments) "
      ],
      "metadata": {
        "id": "GGN4damG7eV7"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and calculate accuracy\n",
        "\n",
        "df_classification = pd.DataFrame() \n",
        "df_classification['Comments'] = test['comment_text']\n",
        "\n",
        "for i,j in enumerate(label.columns):\n",
        "    model,log = get_model(bow,label[j].values)\n",
        "    df_classification[j] = model.predict(bow_test.dot(log))\n",
        "\n",
        "    # calculate accuracy\n",
        "    score = model.score(bow_test.dot(log) , test[j])\n",
        "    print(\"Accuracy for class {} is {}\".format(j, score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe5RZ81AreKF",
        "outputId": "4cb6ef9b-db02-4c47-ef2e-57373c4fe83e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class toxic is 0.9278970896245584\n",
            "Accuracy for class severe_toxic is 0.9937791115696021\n",
            "Accuracy for class obscene is 0.9506549126262153\n",
            "Accuracy for class threat is 0.9960767763918847\n",
            "Accuracy for class insult is 0.9502641533026978\n",
            "Accuracy for class identity_hate is 0.9884647847697646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate F1-score\n",
        "\n",
        "for i,j in enumerate(label.columns):\n",
        "\n",
        "  model,log = get_model(bow,label[j].values)\n",
        "  df_classification[j] = model.predict(bow_test.dot(log))\n",
        "\n",
        "  f1_score = metrics.f1_score(df_classification[j], test[j])\n",
        "  print(\"F1 score for class {} is {}\".format(j, f1_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yEVfsVCBBmK",
        "outputId": "18dbb916-419c-4102-e363-c413e358e329"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for class toxic is 0.5539978729575559\n",
            "F1 score for class severe_toxic is 0.16736401673640167\n",
            "F1 score for class obscene is 0.3443406022845275\n",
            "F1 score for class threat is 0.03088803088803089\n",
            "F1 score for class insult is 0.22841901066925313\n",
            "F1 score for class identity_hate is 0.044041450777202076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TF-IDF + NB +Logistic regression"
      ],
      "metadata": {
        "id": "nvM5h3IJ6roH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word embedding method 2 : TF-IDF \n",
        "\n",
        "# Filter words that appear more than 30% but less than 90% of the document\n",
        "tfidf_vec = TfidfVectorizer(min_df=1,max_df=0.9)\n",
        "\n",
        "# TF-IDF for test\n",
        "tfidf = tfidf_vec.fit_transform(clean_comments)\n",
        "# TF-IDF for train\n",
        "tfidf_test = tfidf_vec.transform(test_clean_comments)"
      ],
      "metadata": {
        "id": "yRLyP5X96mUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model and calcuate accuracy\n",
        "\n",
        "df_classification = pd.DataFrame()\n",
        "df_classification['Comments'] = test['comment_text']\n",
        "\n",
        "for i,j in enumerate(label.columns):\n",
        "    model,log = get_model(tfidf,label[j].values)\n",
        "    df_classification[j] = model.predict(tfidf_test.dot(log))\n",
        "\n",
        "    # Accuracy\n",
        "    score = model.score(tfidf_test.dot(log) , test[j])\n",
        "    print(\"Accuracy for class {} is {}\".format(j, score))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yplOffrD4KF_",
        "outputId": "770e2915-1eec-48a9-d250-29ae427fcf29"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for class toxic is 0.9161743099190347\n",
            "Accuracy for class severe_toxic is 0.994248022757823\n",
            "Accuracy for class obscene is 0.9481853137015849\n",
            "Accuracy for class threat is 0.9967019913095126\n",
            "Accuracy for class insult is 0.9470755572227954\n",
            "Accuracy for class identity_hate is 0.9888711744662227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate F1-score\n",
        "\n",
        "for i,j in enumerate(label.columns):\n",
        "\n",
        "  model,log = get_model(tfidf,label[j].values)\n",
        "  df_classification[j] = model.predict(tfidf_test.dot(log))\n",
        "\n",
        "  f1_score = metrics.f1_score(df_classification[j], test[j])\n",
        "  print(\"F1 score for class {} is {}\".format(j, f1_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMpavIxjEqoy",
        "outputId": "b22a944f-c740-4dcd-c7de-526d84aa38ec"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for class toxic is 0.5187112985730952\n",
            "F1 score for class severe_toxic is 0.0\n",
            "F1 score for class obscene is 0.4075067024128687\n",
            "F1 score for class threat is 0.0\n",
            "F1 score for class insult is 0.2835378755818874\n",
            "F1 score for class identity_hate is 0.0\n"
          ]
        }
      ]
    }
  ]
}