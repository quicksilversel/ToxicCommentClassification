{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../../Data/test.csv')\n",
    "test_labels = pd.read_csv('../../Data/test_labels.csv')\n",
    "test_filtered = pd.merge(test, test_labels)\n",
    "test_filtered = test_filtered.drop(test_filtered.index[test_filtered['toxic'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = pd.read_csv('../CatBoost/catboost_predictions.csv')\n",
    "lreg = pd.read_csv('../LogisticRegression/predictions_csv_files/LR_precisions.csv')\n",
    "nlb = pd.read_csv('../LogisticRegression/predictions_csv_files/NB-LR-BOW_precisions.csv')\n",
    "nlt = pd.read_csv('../LogisticRegression/predictions_csv_files/NB-LR-TFIDF_precisions.csv')\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "models = [bert, lreg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble will perform better if the individual models have low correlations with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "[[1.         0.71737539]\n",
      " [0.71737539 1.        ]]\n",
      "severe_toxic\n",
      "[[1.         0.46522711]\n",
      " [0.46522711 1.        ]]\n",
      "obscene\n",
      "[[1.         0.73659078]\n",
      " [0.73659078 1.        ]]\n",
      "threat\n",
      "[[1.        0.2553561]\n",
      " [0.2553561 1.       ]]\n",
      "insult\n",
      "[[1.         0.65851944]\n",
      " [0.65851944 1.        ]]\n",
      "identity_hate\n",
      "[[1.         0.37771564]\n",
      " [0.37771564 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    print(label)\n",
    "    print(np.corrcoef([x[label] for x in models]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic :\n",
      "0.6607051235065602\n",
      "severe_toxic :\n",
      "0.39948453608247425\n",
      "obscene :\n",
      "0.6679368535655961\n",
      "threat :\n",
      "0.42045454545454547\n",
      "insult :\n",
      "0.6132075471698114\n",
      "identity_hate :\n",
      "0.41651031894934337\n",
      "Average f1-score: 0.5297164874547218\n"
     ]
    }
   ],
   "source": [
    "# Create new predictions based on the other models\n",
    "preds = models[0].copy()\n",
    "preds.shape\n",
    "for label in labels:\n",
    "    sum = 0\n",
    "    for x in models:\n",
    "        sum += x[label]\n",
    "    sum /= len(models)\n",
    "    preds[label] = sum\n",
    "\n",
    "\n",
    "# Calculate f1-score\n",
    "avg = 0\n",
    "for i, label in enumerate(labels):\n",
    "    print(label, \":\")\n",
    "    pb = preds[label] >= 0.5\n",
    "    score = f1_score(test_filtered[label], pb)\n",
    "    print(score)\n",
    "    avg += score\n",
    "\n",
    "avg /= len(labels)\n",
    "print(\"Average f1-score:\", avg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f0756d1539b148037f3e5ac67af8f49cb7b47ce7ee88e1da2562ab72b6a7a25e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
